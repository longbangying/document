1.实验环境
namenode1   xbang1(192.168.153.129)
namenode2   xbang2(192.168.153.130)
datanode1   xbang3(192.168.153.131)
datanode2   xbang4(192.168.153.132)
datanode3   xbang5(192.168.153.133)

zookeeper
xbang3(192.168.153.131)
xbang4(192.168.153.132)
xbang5(192.168.153.133)


操作系统:CentOS-7
JDK: 1.8

hadoop namenode的高可用，其实就是由之前的一个namenode,发展为2个namenode或多个namenode,

1.修改core.site.xml
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/hadoop/tmp</value>
	</property>
	
	<property>
     <name>fs.defaultFS</name>
     <value>hdfs://mycluster</value>
	</property>
	<!--ha.zookeeper.quorum为zookeeper集群的地址-->
	<property>
		<name>ha.zookeeper.quorum</name>
		<value>xbang3:2181,xbang4:2181,xbang5:2181</value>
	</property>	
2.修改hdfs-site.xml
    <property>
		<name>dfs.namenode.name.dir</name>
		<value>/opt/hadoop/dfs/name</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/opt/hadoop/dfs/data</value>
	</property>
	<property>
		<name>replication</name>
		<value>2</value>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
	<!--HA配置 -->
	<property>
		 <name>dfs.nameservices</name>
		 <value>mycluster</value>
	</property>
	<property>
		 <name>dfs.ha.namenodes.mycluster</name>
		 <value>nn1,nn2</value>
	</property>
	<!--namenode1 RPC端口 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn1</name>
		<value>xbang2:9000</value>
	</property>
	<!--namenode1 HTTP端口 -->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn1</name>
		<value>xbang2:50070</value>
	</property>
	<!--namenode2 RPC端口 -->
	<property>
		 <name>dfs.namenode.rpc-address.mycluster.nn2</name>
		 <value>xbang1:9000</value>
	</property>
	<!--namenode1 HTTP端口 -->
	<property>
		 <name>dfs.namenode.http-address.mycluster.nn2</name>
		 <value>xbang1:50070</value>
	</property>
	<!--HA故障切换 -->
	<property>
		 <name>dfs.ha.automic-failover.enabled.cluster</name>
		 <value>true</value>
	</property>
	<!-- journalnode 配置 -->
	<property>
		 <name>dfs.namenode.shared.edits.dir</name>
		 <value>qjournal://xbang3:8485;xbang4:8485;xbang5:8485/mycluster</value>
	</property>
	<property>
		 <name>dfs.client.failover.proxy.provider.mycluster</name>
		 <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
	<!--发生failover时，Standby的节点要执行一系列方法把原来那个Active节点中不健康的NameNode服务给杀掉，
	这个叫做fence过程。sshfence会通过ssh远程调用fuser命令去找到Active节点的NameNode服务并杀死它-->
	<property>
		 <name>dfs.ha.fencing.methods</name>
		 <value>shell(/bin/true)</value>
	</property>
   <!--SSH私钥 -->
  <property>
      <name>dfs.ha.fencing.ssh.private-key-files</name>
      <value>/root/.ssh/id_rsa</value>
  </property>
 <!--SSH超时时间 -->
  <property>
      <name>dfs.ha.fencing.ssh.connect-timeout</name>
      <value>30000</value>
  </property>
  <!--Journal Node文件存储地址 -->
  <property>
      <name>dfs.journalnode.edits.dir</name>
      <value>/opt/hadoop/journal</value>
  </property>

3.其它文件先不修改,单namenode 是什么样还是什么样

启动
1.在其中一台namenode上格式化zookeeper 
	/bin/hdfs zkfc -formatZK
2.启动journalnode集群(datanode1,datanode2,datanode3)
	./sbin/hadoop-daemon.sh  start journalnode
3.在其中一台namenode上格式化namenode
	./bin/hdfs namenode -format
4.启动datanode(datanode1,datanode2,datanode3)
	./sbin/hadoop-daemon.sh start datanode
5.启动namenode
	3中的就直接启动即可
		./sbin/hadoop-daemon.sh start namenode
	其它的因为还没格式化过,因此还需格式化
		./bin/hdfs namenode -bootstrapStandby     //格式化  
		./sbin/hadoop-daemon.sh start namenode
		
此时namenode1和namenode2同时处于standby状态
6.启动zkfc服务
	在namenode1和namenode2上同时执行如下命令   ./sbin/hadoop-daemon.sh  start zkfc   启动zkfc服务后，namenode1和namenode2会自动选举出active节点
	也可手动选举：hdfs haadmin -transitionToActive nn1

7.启动ResourceManager 和nodemanager  
	 ./yarn-daemon.sh start resourcemanager
	 ./yarn-daemon.sh start nodemanager




参考:
https://blog.csdn.net/cl2010abc/article/details/80540280
https://www.cnblogs.com/limaosheng/p/10604701.html